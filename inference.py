#!/usr/bin/env python3
"""
vLLM æ¨ç†è„šæœ¬ - ç®€æ´æ˜“ç”¨çš„æ¨ç†æ¡†æ¶

ä½¿ç”¨æ–¹æ³•:
    python inference.py \\
        --model_name_or_path Qwen/Qwen2.5-7B-Instruct \\
        --input_file data/input.jsonl \\
        --output_file data/output.jsonl \\
        --input_parser examples/input_parser.py \\
        --input_parser_fn parse_to_messages \\
        --output_parser examples/output_parser.py \\
        --output_parser_fn process_output \\
        --num_gpus 4 \\
        --tensor_parallel_size 2
"""

import argparse
import json
import importlib.util
import sys
import os
from pathlib import Path
from typing import List, Dict, Any, Callable
from tqdm import tqdm

from vllm import LLM, SamplingParams


def load_function_from_file(file_path: str, function_name: str) -> Callable:
    """ä»æŒ‡å®šçš„ Python æ–‡ä»¶ä¸­åŠ è½½å‡½æ•°"""
    spec = importlib.util.spec_from_file_location("custom_module", file_path)
    module = importlib.util.module_from_spec(spec)
    sys.modules["custom_module"] = module
    spec.loader.exec_module(module)
    return getattr(module, function_name)


def load_data(input_file: str) -> List[Dict[str, Any]]:
    """è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½ JSON æˆ– JSONL æ ¼å¼çš„æ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read().strip()
    
    # å°è¯•è§£æä¸º JSON æ•°ç»„
    try:
        data = json.loads(content)
        if isinstance(data, list):
            print(f"âœ“ æ£€æµ‹åˆ° JSON æ ¼å¼ï¼ŒåŠ è½½ {len(data)} æ¡æ•°æ®")
            return data
    except json.JSONDecodeError:
        pass
    
    # æŒ‰ JSONL æ ¼å¼è§£æ
    data = []
    for line in content.split('\n'):
        line = line.strip()
        if line:
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"âš  è·³è¿‡æ— æ•ˆè¡Œ: {line[:50]}... é”™è¯¯: {e}")
    
    print(f"âœ“ æ£€æµ‹åˆ° JSONL æ ¼å¼ï¼ŒåŠ è½½ {len(data)} æ¡æ•°æ®")
    return data


def save_output(output_file: str, item: Dict[str, Any], mode: str = 'a'):
    """ä»¥ JSONL æ ¼å¼è¿½åŠ ä¿å­˜å•æ¡æ•°æ®"""
    with open(output_file, mode, encoding='utf-8') as f:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')


def load_checkpoint(checkpoint_file: str) -> int:
    """åŠ è½½æ£€æŸ¥ç‚¹ï¼Œè¿”å›å·²å¤„ç†çš„æ•°æ®æ¡æ•°"""
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            return int(f.read().strip())
    return 0


def save_checkpoint(checkpoint_file: str, processed_count: int):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    with open(checkpoint_file, 'w') as f:
        f.write(str(processed_count))


def run_inference(args):
    """ä¸»æ¨ç†æµç¨‹"""
    
    # 1. åŠ è½½è¾“å…¥è§£æå‡½æ•°å’Œè¾“å‡ºè§£æå‡½æ•°
    print("\n" + "="*60)
    print("ğŸ“¥ åŠ è½½è‡ªå®šä¹‰è§£æå‡½æ•°...")
    input_parser = load_function_from_file(args.input_parser, args.input_parser_fn)
    output_parser = load_function_from_file(args.output_parser, args.output_parser_fn)
    print(f"âœ“ è¾“å…¥è§£æå‡½æ•°: {args.input_parser}::{args.input_parser_fn}")
    print(f"âœ“ è¾“å‡ºè§£æå‡½æ•°: {args.output_parser}::{args.output_parser_fn}")
    
    # 2. åŠ è½½è¾“å…¥æ•°æ®
    print("\n" + "="*60)
    print("ğŸ“‚ åŠ è½½è¾“å…¥æ•°æ®...")
    all_data = load_data(args.input_file)

    # 3. å¤„ç†æ•°æ®åˆ†ç‰‡ï¼ˆæ”¯æŒæ‰‹åŠ¨åˆ†ç‰‡å’Œ torchrun åˆ†ç‰‡ï¼‰
    if args.shard_id is not None and args.num_shards is not None:
        # æ‰‹åŠ¨åˆ†ç‰‡æ¨¡å¼
        shard_id = args.shard_id
        num_shards = args.num_shards
        rank = shard_id
        world_size = num_shards
    else:
        # torchrun æ¨¡å¼ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        rank = int(os.environ.get("RANK", 0))
        world_size = int(os.environ.get("WORLD_SIZE", 1))

    if world_size > 1:
        # æ•°æ®å¹¶è¡Œï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®
        shard_size = len(all_data) // world_size
        start_idx = rank * shard_size
        end_idx = start_idx + shard_size if rank < world_size - 1 else len(all_data)
        data = all_data[start_idx:end_idx]
        print(
            f"âœ“ æ•°æ®åˆ†ç‰‡ {rank}/{world_size}ï¼Œå¤„ç†æ•°æ® [{start_idx}:{end_idx}]ï¼ˆå…± {len(data)} æ¡ï¼‰"
        )
    else:
        data = all_data
        print(f"âœ“ å•è¿›ç¨‹æ¨¡å¼ï¼Œå¤„ç†å…¨éƒ¨ {len(data)} æ¡æ•°æ®")

    # 4. è®¾ç½®è¾“å‡ºæ–‡ä»¶ï¼ˆå¤šåˆ†ç‰‡æ—¶æ·»åŠ åç¼€ï¼‰
    # å¦‚æœæ˜ç¡®æŒ‡å®šäº†åˆ†ç‰‡å‚æ•°ï¼Œæˆ–è€… world_size > 1ï¼Œéƒ½æ·»åŠ åˆ†ç‰‡åç¼€
    if world_size > 1 or (args.shard_id is not None and args.num_shards is not None):
        # ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºæ–‡ä»¶
        base_name = args.output_file.rsplit(".", 1)
        if len(base_name) == 2:
            output_file = f"{base_name[0]}_shard{rank}.{base_name[1]}"
        else:
            output_file = f"{args.output_file}_shard{rank}"
        print(f"âœ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    else:
        output_file = args.output_file

    # 5. æ–­ç‚¹ç»­ä¼ 
    checkpoint_file = f"{output_file}.checkpoint"
    if args.resume:
        processed_count = load_checkpoint(checkpoint_file)
        if processed_count > 0:
            print(f"âœ“ ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œè·³è¿‡å‰ {processed_count} æ¡æ•°æ®")
            data = data[processed_count:]
    else:
        processed_count = 0
        # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶
        open(output_file, "w").close()
    
    if len(data) == 0:
        print("âœ“ æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆ")
        return

    # 5. è®¾ç½® GPU è®¾å¤‡ï¼ˆæ‰‹åŠ¨åˆ†ç‰‡æ¨¡å¼ä¸‹æ ¹æ® shard_id åˆ†é… GPUï¼‰
    if args.shard_id is not None and args.num_shards is not None:
        # æ‰‹åŠ¨åˆ†ç‰‡æ¨¡å¼ï¼šæ ¹æ® shard_id è‡ªåŠ¨åˆ†é… GPU
        gpu_start = args.shard_id * args.tensor_parallel_size
        gpu_ids = list(range(gpu_start, gpu_start + args.tensor_parallel_size))
        os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
        print(f"âœ“ åˆ†ç‰‡ {args.shard_id} ä½¿ç”¨ GPU: {gpu_ids}")
        actual_tp_size = args.tensor_parallel_size
    elif world_size > 1:
        # torchrun æ¨¡å¼ï¼šä»ç¯å¢ƒå˜é‡è·å– rank å¹¶åˆ†é… GPU
        gpu_start = rank * args.tensor_parallel_size
        gpu_ids = list(range(gpu_start, gpu_start + args.tensor_parallel_size))
        os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpu_ids))
        print(f"\nâœ“ Rank {rank} ä½¿ç”¨ GPU: {gpu_ids}")
        actual_tp_size = args.tensor_parallel_size
    else:
        actual_tp_size = args.tensor_parallel_size

    # 6. åˆå§‹åŒ– vLLM æ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸš€ åˆå§‹åŒ– vLLM æ¨¡å‹...")
    print(f"   æ¨¡å‹: {args.model_name_or_path}")
    print(f"   å¼ é‡å¹¶è¡Œ: {actual_tp_size}")
    print(f"   GPU å†…å­˜åˆ©ç”¨ç‡: {args.gpu_memory_utilization}")

    llm = LLM(
        model=args.model_name_or_path,
        tensor_parallel_size=actual_tp_size,
        gpu_memory_utilization=args.gpu_memory_utilization,
        trust_remote_code=args.trust_remote_code,
        max_model_len=args.max_model_len if args.max_model_len > 0 else None,
    )

    tokenizer = llm.get_tokenizer()

    # 7. è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        max_tokens=args.max_tokens,
    )
    
    print(f"âœ“ é‡‡æ ·å‚æ•°: temperature={args.temperature}, top_p={args.top_p}, "
          f"top_k={args.top_k}, max_tokens={args.max_tokens}")

    # 8. æ‰¹é‡æ¨ç†
    print("\n" + "=" * 60)
    print("âš™ï¸  å¼€å§‹æ¨ç†...")

    batch_size = args.batch_size
    total_batches = (len(data) + batch_size - 1) // batch_size

    for batch_idx in tqdm(
        range(0, len(data), batch_size), desc="æ¨ç†è¿›åº¦", total=total_batches
    ):
        batch_data = data[batch_idx : batch_idx + batch_size]

        # è§£æè¾“å…¥æ•°æ®ä¸º messages
        prompts = []
        for item in batch_data:
            try:
                messages = input_parser(item)
                # ä½¿ç”¨ tokenizer çš„ chat template
                prompt = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                prompts.append(prompt)
            except Exception as e:
                print(f"\nâš  è¾“å…¥è§£æå¤±è´¥: {e}")
                prompts.append(None)

        # è¿‡æ»¤æ‰è§£æå¤±è´¥çš„æ•°æ®
        valid_indices = [i for i, p in enumerate(prompts) if p is not None]
        valid_prompts = [prompts[i] for i in valid_indices]
        valid_data = [batch_data[i] for i in valid_indices]

        if not valid_prompts:
            continue

        # æ‰§è¡Œæ¨ç†
        outputs = llm.generate(valid_prompts, sampling_params)

        # å¤„ç†è¾“å‡º
        for item, output in zip(valid_data, outputs):
            response = output.outputs[0].text
            try:
                result_item = output_parser(item, response)
                save_output(output_file, result_item)
            except Exception as e:
                print(f"\nâš  è¾“å‡ºè§£æå¤±è´¥: {e}")

        # æ›´æ–°æ£€æŸ¥ç‚¹
        if args.resume:
            processed_count += len(batch_data)
            save_checkpoint(checkpoint_file, processed_count)

    # 9. æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶
    if args.resume and os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)

    print("\n" + "=" * 60)
    print("âœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="vLLM æ¨ç†è„šæœ¬")

    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "--model_name_or_path", type=str, required=True, help="æ¨¡å‹åç§°æˆ–è·¯å¾„"
    )
    parser.add_argument(
        "--input_file", type=str, required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶ (JSON æˆ– JSONL)"
    )
    parser.add_argument(
        "--output_file", type=str, required=True, help="è¾“å‡ºæ•°æ®æ–‡ä»¶ (JSONL)"
    )
    parser.add_argument(
        "--input_parser",
        type=str,
        required=True,
        help="è¾“å…¥è§£æå‡½æ•°æ‰€åœ¨çš„ Python æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--input_parser_fn", type=str, required=True, help="è¾“å…¥è§£æå‡½æ•°åç§°"
    )
    parser.add_argument(
        "--output_parser",
        type=str,
        required=True,
        help="è¾“å‡ºè§£æå‡½æ•°æ‰€åœ¨çš„ Python æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--output_parser_fn", type=str, required=True, help="è¾“å‡ºè§£æå‡½æ•°åç§°"
    )

    # åˆ†å¸ƒå¼å‚æ•°
    parser.add_argument(
        "--num_gpus", type=int, default=1, help="ä½¿ç”¨çš„ GPU æ€»æ•°ï¼ˆç”¨äºæ•°æ®å¹¶è¡Œè®¡ç®—ï¼‰"
    )
    parser.add_argument(
        "--tensor_parallel_size", type=int, default=1, help="å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆTPï¼‰"
    )

    # æ‰‹åŠ¨æ•°æ®åˆ†ç‰‡å‚æ•°
    parser.add_argument(
        "--shard_id",
        type=int,
        default=None,
        help="å½“å‰åˆ†ç‰‡çš„ IDï¼ˆ0å¼€å§‹ï¼‰ï¼Œç”¨äºæ‰‹åŠ¨æ•°æ®å¹¶è¡Œ",
    )
    parser.add_argument(
        "--num_shards", type=int, default=None, help="æ€»åˆ†ç‰‡æ•°ï¼Œç”¨äºæ‰‹åŠ¨æ•°æ®å¹¶è¡Œ"
    )

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.9,
                        help="GPU å†…å­˜åˆ©ç”¨ç‡")
    parser.add_argument("--trust_remote_code", action="store_true",
                        help="æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ")
    parser.add_argument("--max_model_len", type=int, default=0,
                        help="æœ€å¤§æ¨¡å‹é•¿åº¦ï¼ˆ0 è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼ï¼‰")
    
    # é‡‡æ ·å‚æ•°
    parser.add_argument("--temperature", type=float, default=0.7,
                        help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top_p", type=float, default=0.9,
                        help="top-p é‡‡æ ·")
    parser.add_argument("--top_k", type=int, default=-1,
                        help="top-k é‡‡æ ·ï¼ˆ-1 è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰")
    parser.add_argument("--max_tokens", type=int, default=512,
                        help="æœ€å¤§ç”Ÿæˆ token æ•°")
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--batch_size", type=int, default=32,
                        help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--resume", action="store_true",
                        help="æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤")
    
    args = parser.parse_args()

    # å‚æ•°éªŒè¯
    if (args.shard_id is None) != (args.num_shards is None):
        parser.error("--shard_id å’Œ --num_shards å¿…é¡»åŒæ—¶æŒ‡å®šæˆ–åŒæ—¶ä¸æŒ‡å®š")

    if args.shard_id is not None:
        if args.shard_id < 0 or args.shard_id >= args.num_shards:
            parser.error(f"--shard_id å¿…é¡»åœ¨ [0, {args.num_shards - 1}] èŒƒå›´å†…")
        print(f"\nğŸ”¹ æ‰‹åŠ¨æ•°æ®åˆ†ç‰‡æ¨¡å¼")
        print(f"   å½“å‰åˆ†ç‰‡: {args.shard_id}/{args.num_shards}")
        print(f"   å¼ é‡å¹¶è¡Œ: {args.tensor_parallel_size}")
        print(
            f"   ä½¿ç”¨ GPU: {list(range(args.shard_id * args.tensor_parallel_size, (args.shard_id + 1) * args.tensor_parallel_size))}"
        )
    else:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ torchrun
        data_parallel = args.num_gpus // args.tensor_parallel_size
        if data_parallel > 1:
            print(f"\nâš  æ³¨æ„: æ£€æµ‹åˆ°æ•°æ®å¹¶è¡Œåº¦ä¸º {data_parallel}")
            print(f"   è¯·ä½¿ç”¨ torchrun å¯åŠ¨è„šæœ¬:")
            print(f"   torchrun --nproc_per_node={data_parallel} inference.py ...")
            print(f"   æˆ–ä½¿ç”¨æ‰‹åŠ¨åˆ†ç‰‡æ¨¡å¼: --shard_id 0 --num_shards {data_parallel}")
            print(f"   å½“å‰å°†ä»¥å•è¿›ç¨‹æ¨¡å¼è¿è¡Œ")
    
    run_inference(args)


if __name__ == "__main__":
    main()

